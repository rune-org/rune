{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8d03cb",
   "metadata": {},
   "source": [
    "# Workflow Documentation Generator with DSPy\n",
    "\n",
    "This notebook demonstrates how to use DSPy to optimize a prompt for generating documentation from workflow JSON data. It defines a Semantic Intermediate Representation (SIR), a serializer, a DSPy signature, a custom metric using a Rubric Judge, and runs the GEPA optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a57e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Optional, Literal\n",
    "from pydantic import BaseModel\n",
    "import dspy\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4df30",
   "metadata": {},
   "source": [
    "## 1. Data Models (SIR)\n",
    "Define the Semantic Intermediate Representation (SIR) models. These classes (`SIRWorkflow`, `SIRStep`, `SIROutcome`) represent the simplified structure of a workflow, stripping away UI-specific details from the raw DSL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIROutcome(BaseModel):\n",
    "    \"\"\"Represents where the flow goes next and why.\"\"\"\n",
    "\n",
    "    target_step_name: str\n",
    "    label: str  # e.g., \"Next Step\", \"If True\", \"On Error\"\n",
    "\n",
    "\n",
    "class SIRStep(BaseModel):\n",
    "    \"\"\"A single step in the workflow.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    node_type: str  # Raw node type from DSL\n",
    "    credentials: Optional[str] = None\n",
    "\n",
    "    # The simplified technical details (no noise)\n",
    "    node_config: dict[str, Any]\n",
    "\n",
    "    # Graph connections\n",
    "    parent_step_name: Optional[str] = None\n",
    "    edges: list[SIROutcome] = []\n",
    "\n",
    "\n",
    "class SIRWorkflow(BaseModel):\n",
    "    \"\"\"The complete semantic representation of the workflow.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    name: str\n",
    "    description: Optional[str]\n",
    "    steps: list[SIRStep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4617de9",
   "metadata": {},
   "source": [
    "## 2. Workflow Serializer\n",
    "The `WorkflowSerializer` class is responsible for converting the raw JSON DSL (Domain Specific Language) into the clean SIR format. It handles:\n",
    "- Building adjacency maps for graph traversal.\n",
    "- Resolving node names and types.\n",
    "- Mapping edges to outcomes (Next, True, False, Error).\n",
    "- Cleaning parameters to remove internal IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowSerializer:\n",
    "    def __init__(self, workflow_dsl: dict[str, Any]):\n",
    "        self.dsl = workflow_dsl\n",
    "        self.nodes = {n[\"id\"]: n for n in workflow_dsl.get(\"nodes\", [])}\n",
    "        self.edges = {e[\"id\"]: e for e in workflow_dsl.get(\"edges\", [])}\n",
    "\n",
    "        # Build adjacency maps\n",
    "        self.outgoing_edges: dict[str, list[dict[str, Any]]] = {}\n",
    "        self.incoming_edges: dict[str, list[dict[str, Any]]] = {}\n",
    "\n",
    "        for edge in workflow_dsl.get(\"edges\", []):\n",
    "            src = edge[\"src\"]\n",
    "            dst = edge[\"dst\"]\n",
    "\n",
    "            if src not in self.outgoing_edges:\n",
    "                self.outgoing_edges[src] = []\n",
    "            self.outgoing_edges[src].append(edge)\n",
    "\n",
    "            if dst not in self.incoming_edges:\n",
    "                self.incoming_edges[dst] = []\n",
    "            self.incoming_edges[dst].append(edge)\n",
    "\n",
    "    def serialize(self) -> SIRWorkflow:\n",
    "        \"\"\"Converts the raw DSL into the Semantic Intermediate Representation.\"\"\"\n",
    "\n",
    "        sir_steps = []\n",
    "\n",
    "        for node_data in self.dsl.get(\"nodes\", []):\n",
    "            step = self._process_node(node_data)\n",
    "            sir_steps.append(step)\n",
    "\n",
    "        return SIRWorkflow(\n",
    "            id=self.dsl.get(\"id\", \"\"),\n",
    "            name=self.dsl.get(\"name\", \"Untitled Workflow\"),\n",
    "            description=self.dsl.get(\"description\", \"\"),\n",
    "            steps=sir_steps,\n",
    "        )\n",
    "\n",
    "    def _resolve_target_name(self, edge_id: str) -> str:\n",
    "        if edge_id in self.edges:\n",
    "            edge = self.edges[edge_id]\n",
    "            target_id = edge[\"dst\"]\n",
    "            target_node = self.nodes.get(target_id)\n",
    "            return target_node.get(\"name\", target_id) if target_node else target_id\n",
    "        return edge_id\n",
    "\n",
    "    def _process_node(self, node: dict[str, Any]) -> SIRStep:\n",
    "        node_id = node[\"id\"]\n",
    "        node_name = node.get(\"name\", node_id)\n",
    "        node_type = node.get(\"type\", \"unknown\")\n",
    "\n",
    "        outcomes = []\n",
    "        if node_id in self.outgoing_edges:\n",
    "            for edge in self.outgoing_edges[node_id]:\n",
    "                edge_id = edge[\"id\"]\n",
    "                target_id = edge[\"dst\"]\n",
    "                target_node = self.nodes.get(target_id)\n",
    "                target_name = (\n",
    "                    target_node.get(\"name\", target_id) if target_node else target_id\n",
    "                )\n",
    "\n",
    "                # Determine the label for this outcome\n",
    "                label = edge.get(\"label\", \"Next\")\n",
    "\n",
    "                # Check if this edge is referenced in parameters (e.g. conditional)\n",
    "                params = node.get(\"parameters\", {})\n",
    "                if params.get(\"true_edge_id\") == edge_id:\n",
    "                    condition = params.get(\"condition\") or params.get(\"expression\", \"\")\n",
    "                    label = f\"Condition met: {condition}\"\n",
    "                elif params.get(\"false_edge_id\") == edge_id:\n",
    "                    label = \"Condition not met\"\n",
    "                elif params.get(\"error_edge\") == edge_id:\n",
    "                    label = \"Error\"\n",
    "\n",
    "                # Handle switch node routes\n",
    "                routes = params.get(\"routes\", [])\n",
    "                if isinstance(routes, list) and edge_id in routes:\n",
    "                    try:\n",
    "                        idx = routes.index(edge_id)\n",
    "                        rules = params.get(\"rules\", [])\n",
    "                        if idx < len(rules):\n",
    "                            rule = rules[idx]\n",
    "                            val = rule.get(\"value\", \"\")\n",
    "                            op = rule.get(\"operator\", \"==\")\n",
    "                            comp = rule.get(\"compare\", \"\")\n",
    "                            label = f\"Case: {val} {op} {comp}\"\n",
    "                        else:\n",
    "                            label = \"Default\"\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                outcomes.append(\n",
    "                    SIROutcome(\n",
    "                        target_step_name=target_name,\n",
    "                        label=label,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Determine Previous Step\n",
    "        prev_name = None\n",
    "        if node_id in self.incoming_edges and self.incoming_edges[node_id]:\n",
    "            e = self.incoming_edges[node_id][0]\n",
    "            src_id = e[\"src\"]\n",
    "            src_node = self.nodes.get(src_id)\n",
    "            prev_name = src_node.get(\"name\", src_id) if src_node else src_id\n",
    "\n",
    "        # Clean and Substitute Parameters\n",
    "        params = node.get(\"parameters\", {}).copy()\n",
    "\n",
    "        # Map switch routes to rules\n",
    "        if \"routes\" in params and isinstance(params[\"routes\"], list):\n",
    "            routes = params[\"routes\"]\n",
    "            rules = params.get(\"rules\", [])\n",
    "            if isinstance(rules, list):\n",
    "                new_rules = []\n",
    "                for idx, rule in enumerate(rules):\n",
    "                    if isinstance(rule, dict):\n",
    "                        new_rule = rule.copy()\n",
    "                        if idx < len(routes):\n",
    "                            new_rule[\"target\"] = self._resolve_target_name(routes[idx])\n",
    "                        new_rules.append(new_rule)\n",
    "                params[\"rules\"] = new_rules\n",
    "\n",
    "            # Handle default route\n",
    "            if len(routes) > len(rules):\n",
    "                default_edge = routes[len(rules)]\n",
    "                if default_edge:\n",
    "                    params[\"default_target\"] = self._resolve_target_name(default_edge)\n",
    "\n",
    "        clean_params = self._clean_parameters(params)\n",
    "\n",
    "        # Extract Credentials\n",
    "        creds = node.get(\"credentials\", None)\n",
    "        cred_type = None\n",
    "        if creds and isinstance(creds, dict):\n",
    "            cred_type = creds.get(\"type\")\n",
    "\n",
    "        return SIRStep(\n",
    "            id=node_id,\n",
    "            name=node_name,\n",
    "            node_type=node_type,  # Use raw type\n",
    "            credentials=cred_type,\n",
    "            node_config=clean_params,\n",
    "            parent_step_name=prev_name,\n",
    "            edges=outcomes,\n",
    "        )\n",
    "\n",
    "    def _clean_parameters(self, params: dict[str, Any]) -> dict[str, Any]:\n",
    "        \"\"\"Removes internal IDs or noisy fields and substitutes IDs with Names.\"\"\"\n",
    "        clean = {}\n",
    "        # Remove keys that are not relevant for documentation\n",
    "        keys_to_remove = [\"credentials\", \"position\", \"routes\"]\n",
    "\n",
    "        for k, v in params.items():\n",
    "            if k in keys_to_remove:\n",
    "                continue\n",
    "\n",
    "            if isinstance(v, str):\n",
    "                # If value is an edge ID, replace it with the target node name\n",
    "                if v in self.edges:\n",
    "                    clean[k] = self._resolve_target_name(v)\n",
    "                else:\n",
    "                    clean[k] = v\n",
    "            elif isinstance(v, dict):\n",
    "                clean[k] = self._clean_parameters(\n",
    "                    v\n",
    "                )  # Recursive for nested dicts (e.g. headers)\n",
    "            else:\n",
    "                clean[k] = v\n",
    "\n",
    "        return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56ed5b",
   "metadata": {},
   "source": [
    "## 3. Execution & Testing\n",
    "Load the `data.json` file, run the serializer on the workflows, and print the results. This section tests the entire pipeline from raw data to SIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577225bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"data.json\"\n",
    "\n",
    "if os.path.exists(data_file_path):\n",
    "    with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    serialized_results = []\n",
    "    for item in data:\n",
    "        workflow_data = item.get(\"workflow_data\")\n",
    "        if not workflow_data:\n",
    "            continue\n",
    "\n",
    "        if \"id\" not in workflow_data and \"id\" in item:\n",
    "            workflow_data[\"id\"] = str(item[\"id\"])\n",
    "        if \"name\" not in workflow_data and \"name\" in item:\n",
    "            workflow_data[\"name\"] = item[\"name\"]\n",
    "        if \"description\" not in workflow_data and \"description\" in item:\n",
    "            workflow_data[\"description\"] = item[\"description\"]\n",
    "\n",
    "        try:\n",
    "            serializer = WorkflowSerializer(workflow_data)\n",
    "            sir_workflow = serializer.serialize()\n",
    "            serialized_results.append(sir_workflow.model_dump())\n",
    "        except Exception as e:\n",
    "            print(f\"Error serializing workflow {item.get('id')}: {e}\")\n",
    "\n",
    "    print(f\"Serialized {len(serialized_results)} workflows.\")\n",
    "\n",
    "    # Save results to file\n",
    "    with open(\"serialized_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serialized_results, f, indent=2)\n",
    "    print(\"Saved serialized data to serialized_data.json\")\n",
    "\n",
    "    # Example output of first one\n",
    "    if serialized_results:\n",
    "        print(json.dumps(serialized_results[0], indent=2))\n",
    "else:\n",
    "    print(\"data.json not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363dac",
   "metadata": {},
   "source": [
    "## 4. DSPy Signature\n",
    "Define the `WorkflowDocumentation` signature for the LLM. This tells DSPy what the input (workflow JSON, target audience) and output (markdown report) should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowDocumentation(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Generates documentation for an automation workflow based on its JSON definition\n",
    "    and a specific target audience.\n",
    "    \"\"\"\n",
    "\n",
    "    workflow_json: dict[str, Any] = dspy.InputField(\n",
    "        desc=\"The raw JSON domain-specific language of the workflow.\"\n",
    "    )\n",
    "    target_audience: Literal[\"Technical Developer\", \"Executive Summary\"] = (\n",
    "        dspy.InputField(desc=\"The persona to write for.\")\n",
    "    )\n",
    "\n",
    "    report = dspy.OutputField(\n",
    "        desc=\"The generated documentation report in Markdown format.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66937084",
   "metadata": {},
   "source": [
    "## 5. DSPy Configuration\n",
    "Configure the Language Models (LMs) using Vertex AI credentials.\n",
    "- **Student**: `gemini-2.5-flash-lite` (for generation)\n",
    "- **Judge**: `gemini-2.5-flash` (for evaluation)\n",
    "- **Reflector**: `gemini-3-pro-preview` (for feedback/improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "file_path = os.getenv(\"google_creds_path\")\n",
    "\n",
    "if file_path and os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        vertex_credentials = json.load(file)\n",
    "\n",
    "    # Convert to JSON string\n",
    "    vertex_credentials_json = json.dumps(vertex_credentials)\n",
    "\n",
    "    student = dspy.LM(\n",
    "        \"vertex_ai/gemini-2.5-flash-lite\",\n",
    "        temperature=0,\n",
    "        vertex_credentials=vertex_credentials_json,\n",
    "    )\n",
    "    teacher = dspy.LM(\n",
    "        \"vertex_ai/gemini-2.5-pro\",\n",
    "        temperature=0,\n",
    "        vertex_credentials=vertex_credentials_json,\n",
    "    )\n",
    "    reflector = dspy.LM(\n",
    "        \"vertex_ai/gemini-2.5-pro\",\n",
    "        temperature=0,\n",
    "        vertex_credentials=vertex_credentials_json,\n",
    "    )\n",
    "    dspy.configure(lm=student)\n",
    "else:\n",
    "    print(\"Warning: google_creds_path not found or invalid in .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b10f6d",
   "metadata": {},
   "source": [
    "## 6. Metric Definition: Rubric Judge\n",
    "We define a custom `RubricJudge` signature that acts as a QA Specialist. It evaluates the generated report against the ground truth JSON and the target audience requirements based on a strict 1-10 scoring rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3228590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RubricJudge(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a meticulous QA Specialist evaluating documentation for a workflow automation platform.\n",
    "\n",
    "    YOUR TASK:\n",
    "    Evaluate the 'generated_report' by cross-referencing it against the 'workflow_json' (ground truth)\n",
    "    and checking appropriateness for the 'target_audience'.\n",
    "\n",
    "    EVALUATION PROCESS - Follow these steps IN ORDER:\n",
    "\n",
    "    STEP 1: FACTUAL ACCURACY CHECK (Most Critical)\n",
    "    - List every node mentioned in generated_report\n",
    "    - Verify EACH exists in workflow_json (check \"type\" and \"name\" fields)\n",
    "    - Flag any hallucinated nodes, fabricated parameters, or incorrect connections\n",
    "    - Check if credentials, and execution order match the JSON\n",
    "\n",
    "    STEP 2: COMPLETENESS CHECK\n",
    "    - Are all nodes from workflow_json mentioned?\n",
    "    - Are critical parameters (webhooks, API endpoints, expressions) documented?\n",
    "\n",
    "\n",
    "    STEP 3: AUDIENCE APPROPRIATENESS CHECK\n",
    "    Target Audience Expectations:\n",
    "    - \"executive\" / \"non-technical\": No code snippets, no JSON paths, focus on business outcomes\n",
    "    - \"developer\" / \"technical\": Include node types, expressions, data flow details\n",
    "\n",
    "\n",
    "    STEP 4: FORMATTING & READABILITY CHECK\n",
    "    - Valid Markdown structure (headers, lists render correctly)?\n",
    "    - Logical flow (overview → details → conclusion)?\n",
    "    - Free of orphaned sentences or abrupt endings?\n",
    "\n",
    "    STEP 5: INSIGHT & VALUE CHECK\n",
    "    - Does it explain WHY this workflow exists (business purpose)?\n",
    "    - Does it go beyond listing steps to provide actionable understanding?\n",
    "\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "    SCORING RUBRIC (Apply strictly based on steps above):\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "\n",
    "    1-2 (REJECT - Critical Failure):\n",
    "        • ANY hallucinated node not in workflow_json\n",
    "        • Broken/unrenderable Markdown\n",
    "        • Empty, truncated, or placeholder output\n",
    "        • Completely wrong workflow description\n",
    "\n",
    "    3-4 (MAJOR ISSUES):\n",
    "        • Factually accurate BUT severe audience mismatch\n",
    "          (e.g., code snippets for executives, oversimplified for developers)\n",
    "        • Missing >50% of workflow nodes\n",
    "        • Structurally confusing (no clear sections)\n",
    "\n",
    "    5-6 (ACCEPTABLE - Minimum Viable):\n",
    "        • All nodes accurately documented\n",
    "        • Audience-appropriate language\n",
    "        • Readable structure\n",
    "        • BUT: Merely descriptive (\"Node A sends to Node B\")\n",
    "        • No business context\n",
    "\n",
    "    7-8 (GOOD):\n",
    "        • Fully accurate with all parameters covered\n",
    "        • Strong Markdown formatting with clear sections\n",
    "        • Explains WHAT the workflow accomplishes end-to-end\n",
    "        • Appropriate depth for target audience\n",
    "        • Minor omissions only (e.g., optional parameters)\n",
    "\n",
    "    9-10 (EXEMPLARY):\n",
    "        • Everything in 7-8 PLUS:\n",
    "        • Explains WHY this workflow matters to the business\n",
    "        • Perfect persona voice for the audience\n",
    "        • Zero factual errors, zero missing critical details\n",
    "        • Could be published as official documentation\n",
    "\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "    OUTPUT INSTRUCTIONS:\n",
    "    ═══════════════════════════════════════════════════════════\n",
    "    Provide your integer score and reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    workflow_json: str = dspy.InputField(\n",
    "        desc=\"The source-of-truth workflow definition in JSON format.  All claims must be verified against this.\"\n",
    "    )\n",
    "    target_audience: Literal[\"Technical Developer\", \"Executive Summary\"] = (\n",
    "        dspy.InputField(desc=\"The persona to write for.\")\n",
    "    )\n",
    "    generated_report: str = dspy.InputField(\n",
    "        desc=\"The documentation/report to evaluate.\"\n",
    "    )\n",
    "    reasoning: str = dspy.OutputField(\n",
    "        desc=\"Your step-by-step reasoning process leading to the score.\",\n",
    "    )\n",
    "    score: int = dspy.OutputField(\n",
    "        desc=\"Integer score from 1 to 10 inclusive. Output ONLY the number, no text.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75aa2c",
   "metadata": {},
   "source": [
    "## 7. Metric Function\n",
    "The `normalized_rubric_metric` function wraps the `RubricJudge`. It executes the judge using a teacher model and normalizes the 1-10 score to a 0.0-1.0 float required by DSPy optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_rubric_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    1. Calls the RubricJudge.\n",
    "    2. Parses the 1-10 score.\n",
    "    3. Returns a float between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Extract inputs (Inputs come from 'gold', Output comes from 'pred')\n",
    "\n",
    "    judge = dspy.ChainOfThought(RubricJudge)\n",
    "\n",
    "    with dspy.context(lm=teacher):\n",
    "        assessment = judge(\n",
    "            workflow_json=gold.workflow_json,\n",
    "            target_audience=gold.target_audience,\n",
    "            generated_report=pred.report,\n",
    "        )\n",
    "\n",
    "    # 2. Parse and Normalize\n",
    "    try:\n",
    "        raw_score = float(assessment.score)\n",
    "\n",
    "        # Clamp score to ensure it is 1-10 (just in case)\n",
    "        raw_score = max(1, min(10, raw_score))\n",
    "\n",
    "        # Normalize to 0.0 - 1.0\n",
    "        normalized_score = raw_score / 10.0\n",
    "\n",
    "    except ValueError:\n",
    "        # If the LLM output \"7/10\" or \"Score: 7\", parse it or fail safe\n",
    "        normalized_score = 0.0\n",
    "        print(f\"Metric Failed to parse score: {assessment.score}\")\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cbe10",
   "metadata": {},
   "source": [
    "## 8. Dataset Preparation\n",
    "We create a training set from the serialized workflows. For each workflow, we create two examples: one for an \"Executive Summary\" audience and one for a \"Technical Developer\" audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "\n",
    "for workflow in serialized_results:\n",
    "    # Create example for Executive Summary\n",
    "    training_set.append(\n",
    "        dspy.Example(\n",
    "            workflow_json=workflow, target_audience=\"Executive Summary\", report=\"\"\n",
    "        ).with_inputs(\"workflow_json\", \"target_audience\")\n",
    "    )\n",
    "\n",
    "    # Create example for Technical Developer\n",
    "    training_set.append(\n",
    "        dspy.Example(\n",
    "            workflow_json=workflow, target_audience=\"Technical Developer\", report=\"\"\n",
    "        ).with_inputs(\"workflow_json\", \"target_audience\")\n",
    "    )\n",
    "\n",
    "print(f\"Populated training_set with {len(training_set)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2af96",
   "metadata": {},
   "source": [
    "## 9. Optimization with GEPA\n",
    "We use the Genetically Evolutionary Prompt Authorization (GEPA) optimizer to improve the `WorkflowDocumentation` signature. The optimizer uses the `reflector` model to propose improvements and the `normalized_rubric_metric` to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=normalized_rubric_metric,\n",
    "    reflection_lm=reflector,  # The \"Brain\" that reads feedback and rewrites prompts\n",
    "    auto=\"medium\",  # Budget: 'light', 'medium', or 'heavy'\n",
    "    num_threads=16,\n",
    "    # reflection_minibatch_size=10,\n",
    ")\n",
    "bot = dspy.ChainOfThought(WorkflowDocumentation)\n",
    "compiled_gepa = optimizer.compile(\n",
    "    bot,\n",
    "    trainset=training_set,\n",
    ")\n",
    "compiled_gepa.save(\"rune_optimized_prompt.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
